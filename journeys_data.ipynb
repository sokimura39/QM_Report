{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4e368c-26d2-442c-8d12-eb58ba353e84",
   "metadata": {},
   "source": [
    "# Fetching the journeys data for analysis\n",
    "\n",
    "Download the CSV data from TfL, and save in a GeoPandas geodataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f68f977-24b1-4f7d-a47a-26d2a96c1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts journeys into geometry\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import urllib.request\n",
    "from requests import get\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3246b48-fe9b-43e7-9101-8fa8cf60a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "DL_path = \"data/cycles/DL_data\"\n",
    "points_path = \"data/cycles/points\"\n",
    "points_fn = \"BikePoints.geojson\"\n",
    "journeys_path = \"data/cycles/journeys\"\n",
    "\n",
    "# set source URL and filename\n",
    "source_url = \"https://cycling.data.tfl.gov.uk/usage-stats/\"\n",
    "source_fn = [\n",
    "    \"346JourneyDataExtract28Nov2022-04Dec2022.csv\",\n",
    "    \"347JourneyDataExtract05Dec2022-11Dec2022.csv\",\n",
    "    \"348JourneyDataExtract12Dec2022-18Dec2022.csv\",\n",
    "    \"349JourneyDataExtract19Dec2022-25Dec2022.csv\",\n",
    "    \"350JourneyDataExtract26Dec2022-01Jan2023.csv\",\n",
    "    \"351JourneyDataExtract02Jan2023-08Jan2023.csv\",\n",
    "    \"352JourneyDataExtract09Jan2023-15Jan2023.csv\",\n",
    "    \"353JourneyDataExtract16Jan2023-22Jan2023.csv\",\n",
    "    \"354JourneyDataExtract23Jan2023-29Jan2023.csv\",\n",
    "    \"355JourneyDataExtract30Jan2023-05Feb2023.csv\",\n",
    "    \"356JourneyDataExtract06Feb2023-12Feb2023.csv\",\n",
    "    \"357JourneyDataExtract13Feb2023-19Feb2023.csv\",\n",
    "    \"358JourneyDataExtract20Feb2023-26Feb2023.csv\",\n",
    "    \"359JourneyDataExtract27Feb2023-05Mar2023.csv\",\n",
    "    \"360JourneyDataExtract06Mar2023-12Mar2023.csv\",\n",
    "    \"361JourneyDataExtract13Mar2023-19Mar2023.csv\",\n",
    "    \"362JourneyDataExtract20Mar2023-26Mar2023.csv\",\n",
    "    \"363JourneyDataExtract27Mar2023-02Apr2023.csv\",\n",
    "    \"364JourneyDataExtract03Apr2023-09Apr2023.csv\",\n",
    "    \"365JourneyDataExtract10Apr2023-16Apr2023.csv\",\n",
    "    \"366JourneyDataExtract17Apr2023-23Apr2023.csv\",\n",
    "    \"367JourneyDataExtract24Apr2023-30Apr2023.csv\",\n",
    "    \"368JourneyDataExtract01May2023-07May2023.csv\",\n",
    "    \"369JourneyDataExtract08May2023-14May2023.csv\",\n",
    "    \"370JourneyDataExtract15May2023-21May2023.csv\",\n",
    "    \"371JourneyDataExtract22May2023-28May2023.csv\",\n",
    "    \"372JourneyDataExtract29May2023-04Jun2023.csv\",\n",
    "    \"373JourneyDataExtract05Jun2023-11Jun2023.csv\",\n",
    "    \"374JourneyDataExtract12Jun2023-18Jun2023.csv\",\n",
    "    \"375JourneyDataExtract19Jun2023-30Jun2023.csv\",\n",
    "    \"376JourneyDataExtract01Jul2023-14Jul2023.csv\",\n",
    "    \"377JourneyDataExtract15Jul2023-31Jul2023.csv\",\n",
    "    \"378JourneyDataExtract01Aug2023-14Aug2023.csv\",\n",
    "    \"378JourneyDataExtract15Aug2023-31Aug2023.csv\",\n",
    "    \"379JourneyDataExtract01Sep2023-14Sep2023.csv\",\n",
    "    \"380JourneyDataExtract15Sep2023-30Sep2023.csv\",\n",
    "    \"381JourneyDataExtract01Oct2023-14Oct2023.csv\",\n",
    "    \"382JourneyDataExtract15Oct2023-31Oct2023.csv\",\n",
    "    \"383JourneyDataExtract01Nov2023-14Nov2023.csv\",\n",
    "    \"384JourneyDataExtract15Nov2023-30Nov2023.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f6ad17-6c94-4ca5-8b8d-0dc0e1bb0b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in local: 346JourneyDataExtract28Nov2022-04Dec2022.csv\n",
      "Found in local: 347JourneyDataExtract05Dec2022-11Dec2022.csv\n",
      "Found in local: 348JourneyDataExtract12Dec2022-18Dec2022.csv\n",
      "Found in local: 349JourneyDataExtract19Dec2022-25Dec2022.csv\n",
      "Found in local: 350JourneyDataExtract26Dec2022-01Jan2023.csv\n",
      "Found in local: 351JourneyDataExtract02Jan2023-08Jan2023.csv\n",
      "Found in local: 352JourneyDataExtract09Jan2023-15Jan2023.csv\n",
      "Found in local: 353JourneyDataExtract16Jan2023-22Jan2023.csv\n",
      "Found in local: 354JourneyDataExtract23Jan2023-29Jan2023.csv\n",
      "Found in local: 355JourneyDataExtract30Jan2023-05Feb2023.csv\n",
      "Found in local: 356JourneyDataExtract06Feb2023-12Feb2023.csv\n",
      "Found in local: 357JourneyDataExtract13Feb2023-19Feb2023.csv\n",
      "Found in local: 358JourneyDataExtract20Feb2023-26Feb2023.csv\n",
      "Found in local: 359JourneyDataExtract27Feb2023-05Mar2023.csv\n",
      "Found in local: 360JourneyDataExtract06Mar2023-12Mar2023.csv\n",
      "Found in local: 361JourneyDataExtract13Mar2023-19Mar2023.csv\n",
      "Found in local: 362JourneyDataExtract20Mar2023-26Mar2023.csv\n",
      "Found in local: 363JourneyDataExtract27Mar2023-02Apr2023.csv\n",
      "Found in local: 364JourneyDataExtract03Apr2023-09Apr2023.csv\n",
      "Found in local: 365JourneyDataExtract10Apr2023-16Apr2023.csv\n",
      "Found in local: 366JourneyDataExtract17Apr2023-23Apr2023.csv\n",
      "Found in local: 367JourneyDataExtract24Apr2023-30Apr2023.csv\n",
      "Found in local: 368JourneyDataExtract01May2023-07May2023.csv\n",
      "Found in local: 369JourneyDataExtract08May2023-14May2023.csv\n",
      "Found in local: 370JourneyDataExtract15May2023-21May2023.csv\n",
      "Found in local: 371JourneyDataExtract22May2023-28May2023.csv\n",
      "Found in local: 372JourneyDataExtract29May2023-04Jun2023.csv\n",
      "Found in local: 373JourneyDataExtract05Jun2023-11Jun2023.csv\n",
      "Found in local: 374JourneyDataExtract12Jun2023-18Jun2023.csv\n",
      "Found in local: 375JourneyDataExtract19Jun2023-30Jun2023.csv\n",
      "Found in local: 376JourneyDataExtract01Jul2023-14Jul2023.csv\n",
      "Found in local: 377JourneyDataExtract15Jul2023-31Jul2023.csv\n",
      "Found in local: 378JourneyDataExtract01Aug2023-14Aug2023.csv\n",
      "Found in local: 378JourneyDataExtract15Aug2023-31Aug2023.csv\n",
      "Found in local: 379JourneyDataExtract01Sep2023-14Sep2023.csv\n",
      "Found in local: 380JourneyDataExtract15Sep2023-30Sep2023.csv\n",
      "Found in local: 381JourneyDataExtract01Oct2023-14Oct2023.csv\n",
      "Found in local: 382JourneyDataExtract15Oct2023-31Oct2023.csv\n",
      "Found in local: 383JourneyDataExtract01Nov2023-14Nov2023.csv\n",
      "Found in local: 384JourneyDataExtract15Nov2023-30Nov2023.csv\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# download journeys\n",
    "\n",
    "# creates saving directory if does not exist\n",
    "if not os.path.exists(journeys_path):\n",
    "    os.makedirs(journeys_path)\n",
    "\n",
    "for fn in source_fn:\n",
    "    # check if local exists\n",
    "    if not os.path.exists(os.path.join(journeys_path, fn)):\n",
    "        source = source_url + fn\n",
    "        print(f\"Downloading   : {fn}\")\n",
    "        with open(os.path.join(journeys_path, fn), \"wb\") as file:\n",
    "            response = get(source)\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(f\"Found in local: {fn}\")\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a208e9ec-c61b-4ce2-a529-32b1922385ab",
   "metadata": {},
   "source": [
    "## Create dataframe from journeys\n",
    "\n",
    "Using the above dataset, a dataframe of journeys are created in this script below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee069c6b-bd55-4147-9232-9124a30f59fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11.2 s\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# test flag, set 1 for testing\n",
    "test_flag = 0\n",
    "\n",
    "\n",
    "journeys_df = []\n",
    "# load data\n",
    "# keeping it as a list to avoid memory crashes in further analysis\n",
    "# merging will come at the very end\n",
    "for idx, fn in enumerate(source_fn):\n",
    "    journeys_df.append(pd.read_csv(os.path.join(journeys_path, fn), low_memory = False))\n",
    "    if test_flag == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d53e64a-6762-4110-9719-da6fb2c18e33",
   "metadata": {},
   "source": [
    "## Join with height data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a9b1d71-fe87-4b94-ab96-3e26d2661b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "points_path = 'data/cycles/points'\n",
    "points_parquet_fn = 'docking_stations.geoparquet'\n",
    "journeys_df_fn = 'journeys.parquet'\n",
    "\n",
    "# load points gdf\n",
    "points_gdf = gpd.read_parquet(os.path.join(points_path, points_parquet_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aed82f8-ff0a-4d87-af24-92b1fe463087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the points data to the journeys data\n",
    "\n",
    "# add prefix to data points\n",
    "start_points = points_gdf.add_prefix('start_')\n",
    "end_points = points_gdf.add_prefix('end_')\n",
    "\n",
    "# create new list\n",
    "journeys_df_merged = []\n",
    "\n",
    "for df in journeys_df:     \n",
    "    # merge the start point data\n",
    "    temp_df = df.merge(start_points, left_on = 'Start station', right_on = 'start_name')\n",
    "    # merge the end point data\n",
    "    temp_df = temp_df.merge(end_points, left_on = 'End station', right_on = 'end_name')\n",
    "\n",
    "    journeys_df_merged.append(temp_df)\n",
    "\n",
    "# delete the unnneccesary df\n",
    "del(start_points)\n",
    "del(end_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74211f72-9e9e-464c-8a10-a6cc72e47ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the required columns\n",
    "columns = ['Number', 'Start date', 'Start station number', 'start_location', 'start_zone', \n",
    "           'start_LSOA11CD', 'start_LSOA11NM', 'start_cc_zone', 'start_height', 'start_geometry',\n",
    "           'End date', 'End station number', 'end_location', 'end_zone', \n",
    "           'end_LSOA11CD', 'end_LSOA11NM', 'end_cc_zone', 'end_height', 'end_geometry',\n",
    "           'Bike number', 'Bike model', 'Total duration (ms)']\n",
    "\n",
    "for df in journeys_df_merged:\n",
    "    df = df[columns].copy()\n",
    "    # clean dates\n",
    "    dates = ['Start date', 'End date']\n",
    "    for d in dates:\n",
    "        df[d] = pd.to_datetime(df[d], format = '%Y-%m-%d %H:%M')\n",
    "    \n",
    "    # make bike model into categorical data\n",
    "    # this should only have 2 types: 'CLASSIC' and 'PBSC_EBIKE'\n",
    "    df['Bike model'] = df['Bike model'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "816f572b-6c0e-421b-bf8a-17770a2a21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one large df from the list of df\n",
    "df_concat = pd.concat(journeys_df_merged, ignore_index = True)\n",
    "\n",
    "# delete the unnneccesary df\n",
    "del(journeys_df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f96feab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get geometry\n",
    "df_concat['geometry'] = df_concat.apply(lambda row: LineString([row['start_geometry'], row['end_geometry']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13db471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn into geodataframe\n",
    "journeys_gdf = gpd.GeoDataFrame(df_concat.drop(['start_geometry', 'end_geometry'], axis = 1), crs = 'EPSG:27700')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a658f96c-3541-461e-a1c5-cacfee75f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the height difference\n",
    "journeys_gdf['height_diff'] = journeys_gdf.end_height - journeys_gdf.start_height\n",
    "\n",
    "# save the distance of journey as a column\n",
    "journeys_gdf['distance'] = journeys_gdf.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18752b32-456a-4487-947c-40713a991239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8.491612e+06\n",
       "mean    -2.200772e-01\n",
       "std      9.166256e+00\n",
       "min     -4.387600e+01\n",
       "25%     -4.118500e+00\n",
       "50%      0.000000e+00\n",
       "75%      3.757999e+00\n",
       "max      4.387600e+01\n",
       "Name: height_diff, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journeys_gdf.height_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79123dbb-fb89-4319-9235-60956fbebdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "journeys_gdf_fn = 'journeys_gdf.geoparquet'\n",
    "\n",
    "journeys_gdf.drop(['Start station number', 'End station number'], axis = 1).to_parquet(os.path.join(journeys_path, journeys_gdf_fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
